Title: Neighbors isometric embedding nonnegative matrix factorization for image representation
Abstract: In this paper, a novel regularized nonnegative matrix factorization (NMF) method, called neighbors isometric embedding nonnegative matrix factorization (NINMF), is proposed. The key idea of the NINMF method is to incorporate neighbors isometric regularized constraint in the optimization of the NMF. Hence, the NINMF is able to extract the representation space that preserves neighbor isometric geometry structure. Like most of the graph regularized NMFs, the NINMF method also finds a similarity weights matrix. However, the difference of our proposed method is that the NINMF simultaneously builds similarity weight matrix and performs data representation. The proposed method was applied to solve the problem of image representation using the well-known ORL, Yale and extended YaleB image data sets. The experimental results demonstrate the effectiveness of the proposed NINMF method for image representation.

Title: Neighbors isometric embedding nonnegative matrix factorization for image representation
Abstract: In this paper, a novel regularized nonnegative matrix factorization (NMF) method, called neighbors isometric embedding nonnegative matrix factorization (NINMF), is proposed. The key idea of the NINMF method is to incorporate neighbors isometric regularized constraint in the optimization of the NMF. Hence, the NINMF is able to extract the representation space that preserves neighbor isometric geometry structure. Like most of the graph regularized NMFs, the NINMF method also finds a similarity weights matrix. However, the difference of our proposed method is that the NINMF simultaneously builds similarity weight matrix and performs data representation. The proposed method was applied to solve the problem of image representation using the well-known ORL, Yale and extended YaleB image data sets. The experimental results demonstrate the effectiveness of the proposed NINMF method for image representation.

Title: Sparse discriminating neighborhood preserving embedding
Abstract: Dimensionality reduction (DR) methods have commonly been used as a principled way to process the high-dimensional data such as face images. In this paper, a novel linear DR method called discriminating neighborhood preserving embedding (DNPE), which incorporates between-class scatter matrix and within-class scatter matrix into neighborhood preserving embedding (NPE), is proposed. It has been shown that DNPE has stronger discriminating power than NPE does. Meanwhile, this paper also proposes sparse discriminating neighborhood preserving embedding (SDNPE) based on sparse representation theory, which directly generates the weight matrix without constructing adjacency graphs. Experimental results on Yale, ORL, AR and Extended YaleB face databases verify the efficacy of the proposed methods.

Title: Manifold Adaptive Label Propagation for Face Clustering
Abstract: In this paper, a novel label propagation (LP) method is presented, called the manifold adaptive label propagation (MALP) method, which is to extend original LP by integrating sparse representation constraint into regularization framework of LP method. Similar to most LP, first of all, MALP also finds graph edges from given data and gives weights to the graph edges. Our goal is to find graph weights matrix adaptively. The key advantage of our approach is that MALP simultaneously finds graph weights matrix and predicts the label of unlabeled data. This paper also derives efficient algorithm to solve the proposed problem. Extensions of our MALP in kernel space and robust version are presented. The proposed method has been applied to the problem of semi-supervised face clustering using the well-known ORL, Yale, extended YaleB, and PIE datasets. Our experimental evaluations show the effectiveness of our method.

Title: Directional Two-dimensional Neighborhood Preserving Projection for Face Recognition
Abstract: This paper presents a novel manifold learning method, namely Directional two-dimensional neighborhood preserving embedding (Dir-2DNPE), for feature extraction. In contrast to standard NPE, Dir-2DNPE directly seeks the optimal projective vectors from the directional images without image-to-vector transformation. Moreover, Dir-2DNPE can well reserve the spatial correlations between variations of rows and those of columns of images. Experiments on the ORL and Yale databases show the effectiveness of the proposed method.

Title: Fast Fisher Sparsity Preserving Projections
Abstract: Recently, there has been a lot of interest in the underlying sparse representation structure in high-dimensional data such as face images. In this paper, we propose two novel efficient dimensionality reduction methods named Fast Sparsity Preserving Projections (FSPP) and Fast Fisher Sparsity Preserving Projections (FFSPP), respectively, which aim to preserve the sparse representation structure in high-dimensional data. Unlike the existing Sparsity Preserving Projections (SPP), where the sparse representation structure is learned through resolving n (the number of samples) time-consuming \( \ell^{ 1} \) norm optimization problems, FSPP constructs a dictionary through classwise PCA decompositions and learns the sparse representation structure under the constructed dictionary through matrix–vector multiplications, which is much more computationally tractable. FFSPP takes into consideration both the sparse representation structure and the discriminating efficiency by adding the Fisher constraint to the FSPP formulation to improve FSPP’s discriminating ability. Both of the proposed methods can boil down to a generalized eigenvalue problem. Experimental results on three publicly available face data sets (Yale, Extended Yale B and ORL), and a standard document collection (Reuters-21578) validate the feasibility and effectiveness of the proposed methods.

Title: The analysis of parameters t and k of LPP on several famous face databases
Abstract: The subspace transformation plays an important role in the face recognition. LPP, which is so-called the Laplacianfaces, is a very popular manifold subspace transformation for face recognition, and it aims to preserve the local structure of the samples. Recently, many variants of LPP are proposed. LPP is a baseline in their experiments. LPP uses the adjacent graph to preserve the local structure of the samples. In the original version of LPP, the local structure is determined by the parameters t (the heat kernel) and k (k-nearest neighbors) and directly influences on the performance of LPP. To the best of our knowledge, there is no report on the relation between the performance and these two parameters. The objective of this paper is to reveal this relation on several famous face databases, i.e. ORL, Yale and YaleB.

Title: Nonnegative matrix factorization on orthogonal subspace with smoothed l0 norm constrained
Abstract: It is known that the sparseness of the factor matrices by Nonnegative Matrix Factorization can influence the clustering performance. In order to improve the ability of the sparse representations of the NMF, we proposed the new algorithm for Nonnegatie Matrix Factorization, coined nonnegative matrix factorization on orthogonal subspace with smoothed L0 norm constrained, in which the generation of orthogonal factor matrices with smoothed L0 norm constrained are the parts of objective function minimization. Also we develop simple multiplicative updates for our proposed method. Experiment on three real-world databases (Iris, UCI, ORL) show that our proposed method can achieve the best or close to the best in clustering and in the way of the sparse representation than other methods.

Title: Dual Locality Preserving Nonnegative Matrix Factorization for image analysis
Abstract: Recently, Nonnegative Matrix Factorization(NMF) has been viewed as an effective method for data engineering for its part-based interpretability and superior performance. However, ordinary NMF merely views a r 1  × r 2  image as a vector in r 1  × r 2  dimensional space and the pixels of the image are considered independent. It fails to consider that an image is intrinsically a matrix, and pixels spatially close to each other may also be correlated in the final learned representation. In this paper, I construct a novel spatially nearest graph and propose a novel algorithm named Dual Locality Preserving Nonnegative Matrix Factorization (DLPNMF), which explicitly models both the spatial correlation between neighboring pixels inside images and the geometric structure among different image vectors. A multiplicative rule is also proposed to solve the corresponding optimization problem. The encouraging experimental results on benchmark image data have demonstrated the effectiveness of this algorithm.

Title: Adaptive graph regularized Nonnegative Matrix Factorization via feature selection
Abstract: Nonnegative Matrix Factorization (NMF), a popular compact data representation method, fails to discover the intrinsic geometrical structure of the data space. Graph regularized NMF (GrNMF) is proposed to avoid this limitation by regularizing NMF with a nearest neighbor graph constructed from the input data feature space. However using the original feature space directly is not appropriate because of the noisy and irrelevant features. In this paper, we propose a novel data representation algorithm by integrating feature selection and graph regularization for NMF. Instead of using a fixed graph as GrNMF, we regularize NMF with an adaptive graph constructed according to the feature selection results. A uniform object is built to consider feature selection, NMF and adaptive graph regularization jointly, and a novel algorithm is developed to update the graph, feature weights and factorization parameters iteratively. Data clustering experiment shows the efficacy of the proposed method on the Yale database.

Title: Local similarity and diversity preserving discriminant projection for face and handwriting digits recognition
Abstract: In this paper, a novel supervised subspace learning algorithm, named local similarity and diversity preserving discriminant projection (LSDDP), is presented. LSDDP defines two weighted adjacency graphs, namely similarity graph and diversity graph. LSDDP constructs the similarity scatter and diversity scatter with the weights, which are adjustable according to the global supervisor and the local semi-supervisor information of the data. Thus LSDDP could utilize both the similarity and diversity information of the data simultaneously for dimensionality reduction. After characterizing the similarity scatter and diversity scatter, a concise feature extraction criterion arised via minimizing the difference between them and the optimal projection is obtained by performing the eigen-decomposition. Thus our method successfully addresses the SSS problem without losing any discriminating information. Finally the proposed model is verified by the face and handwriting digits recognition experiments. The experimental results on Yale, ORL and CMU-PIE face database and the USPS handwriting digits database indicate the effectiveness of our method.

Title: Letters: An alternative formulation of kernel LPP with application to image recognition
Abstract: Locality preserving projections (LPP) is a new subspace feature extraction method which seeks to preserve the local structure and intrinsic geometry of the data space. As the LPP model is linear, it may fail to extract the nonlinear features. This paper proposes to address this problem using an alternative formulation, kernel locality preserving projections (KLPP). Our algorithm consists of two steps: kernel principal component analysis (KPCA) plus LPP. We provide an outline for implementing KLPP. Experiments on the ORL face database and PolyU palmprint database demonstrate the effectiveness of the proposed algorithm.

Title: Manifold sparsity preserving projection for face and palmprint recognition
Abstract: Sparsity Preserving Projection (SPP) has been recently successfully applied on pattern recognition applications and is the basis for a series of follow up extensions. However, being unsupervised for dimensionality reduction, SPP does not employ the discriminative information of class labels when projecting data into a smaller subspace. This paper proposes a manifold sparsity learning method called Manifold Sparsity Preserving Projection (MSPP) for the face and palmprint recognition. Our method employs the manifold structure for better preserving the sparsity of data in the embedding space. Differing from recent localized sparsity learning methods such as Local Sparse Representation Projections (LSRP) and Local Sparse Preserving Projections (LSPP), which enforce a one-to-one matching between a sample and its sparsely reconstructed model, our method employs manifold data structure to ensure that a sample and all its classmate’s sparsely reconstructed models remain as close as possible in the new space. We show that when manifold and sparsity information are simultaneously accounted for, their discriminative power is significantly leveraged. An immediate bonus of our approach is that there is no tuning parameter whatsoever for performance variation. We analytically demonstrate the aforementioned features of our approach and then, using a series of experiments on ORL, Yale, IIT Delhi near infrared facial, and NIR set of PolyU Multispectral palmprint databases its pragmatic consequences will be pictured.

Title: Doubly weighted nonnegative matrix factorization for imbalanced face recognition
Abstract: We propose in this paper a novel doubly weighted nonnegative matrix factorization (DWNMF) method for imbalanced face recognition. Motivated by the fact that some face samples and certain parts of each face sample are more useful for recognition, we construct two weighted matrices based on the pairwise similarity of face samples in the same class and the discriminant score of each face pixel. Compared with the existing NMF algorithm, the proposed DWNMF method can more effectively exploit the discriminative and geometrical information of face samples, and it is especially suitable for imbalanced face recognition. Experimental results are presented to demonstrate the efficacy of the proposed method.

Title: Discriminant Sparsity Preserving Analysis for Face Recognition
Abstract: Sparse subspace learning has drawn more and more attentions recently, however, most of them are unsupervised and unsuitable for classification tasks. In this paper, a new discriminant sparsity preserving analysis (DSPA) method by integrating sparse reconstructive weighting into Fisher criterion is proposed for face recognition. We first get sparsity preserving space spanned by the eigenvectors of sparsity preserving projections (SPP). Then, the optimal projection can be obtained by solving an eigenvalue and eigenvector problem of the between-class scatter matrix in sparsity preserving space. The method not only preserves the sparse reconstructive relationship of the data, but also encodes the discriminant information. Extensive experiments on four face image datasets (Yale, ORL, AR and CMU PIE) demonstrate the effectiveness of the proposed DSPA method.

Title: A new subspace analysis approach based on laplacianfaces
Abstract: A new subspace analysis approach named ANLBM is proposed based on Laplacianfaces. It uses the discriminant information of training samples by supervised mechanism, enhances within-class local information by an objective function. The objective function is used to construct adjacency graph's weight matrix. In order to avoid the drawback of Laplacianfaces' PCA step, ANLBM uses kernel mapping. ANLBM changes the problem from minimum eigenvalue solution to maximum eigenvalue solution, reduces the redundancy of the computing and increases the precision of the result. The experiments are performed on ORL and Yale databases. Experimental results show that ANLBM has a better performance.

Title: Convex-Nonnegative Matrix Factorization with structure constraints
Abstract: Nonnegative Matrix Factorization (NMF) is of great use in finding basis information of non-negative data. In this paper, a novel Convex-NMF (CNMF) method is presented, called Structure Constrained Convex-Nonnegative Matrix Factorization (SCNMF). The idea of SCNMF is to extend the original Convex-NMF by incorporating the structure constraints into the Convex-NMF decomposition. The SCNMF seeks to extract the representation space that preserves the geometry structure. Finally, our experiment results are presented.

Title: Sparse manifold embedding Tri-factor Nonnegative Matrix Factorization
Abstract: Tri-factor Nonnegative Matrix Factorization (TNMF) is of use in simultaneously clustering rows and columns of the input data matrix. In this paper, we present a Sparse Manifold Embedding Tri-factor Nonnegative Matrix Factorization (STNMF) for data clustering. Similar to most graph regularized NMF, STNMF is to extend the original TNMF by incorporating the graph regularized and sparse manifold embedding constraints into the TNMF model. The key advantage of this method is that the STNMF simultaneously compute sparse similarity matrix, clustering rows and columns of the input data matrix. Finally, our experiment results are presented.

Title: Manifold regularized non-negative matrix factorization with label information
Abstract: Non-negative matrix factorization (NMF) as a popular technique for finding parts-based, linear representations of non-negative data has been successfully applied in a wide range of applications, such as feature learning, dictionary learning, and dimensionality reduction. However, both the local manifold regularization of data and the discriminative information of the available label have not been taken into account together in NMF. We propose a new semisupervised matrix decomposition method, called manifold regularized non-negative matrix factorization (MRNMF) with label information, which incorporates the manifold regularization and the label information into the NMF to improve the performance of NMF in clustering tasks. We encode the local geometrical structure of the data space by constructing a nearest neighbor graph and enhance the discriminative ability of different classes by effectively using the label information. Experimental comparisons with the state-of-the-art methods on theCOIL20, PIE, Extended Yale B, and MNIST databases demonstrate the effectiveness of MRNMF.

Title: Algorithms for orthogonal nonnegative matrix factorization
Abstract: Nonnegative matrix factorization (NMF) is a widely-used method for multivariate analysis of nonnegative data, the goal of which is decompose a data matrix into a basis matrix and an encoding variable matrix with all of these matrices allowed to have only nonnegative elements. In this paper we present simple algorithms for orthogonal NMF, where orthogonality constraints are imposed on basis matrix or encoding matrix. We develop multiplicative updates directly from the true gradient (natural gradient) in Stiefel manifold, whereas existing algorithms consider additive orthogonality constraints. Numerical experiments on face image data for a image representation task show that our orthogonal NMF algorithm preserves the orthogonality, while the goodness-of-fit (GOF) is minimized. We also apply our orthogonal NMF to a clustering task, showing that it works better than the original NMF, which is confirmed by experiments on several UCI repository data sets.

Title: Non-negative matrix factorization on Kernels
Abstract: In this paper, we extend the original non-negative matrix factorization (NMF) to kernel NMF (KNMF). The advantages of KNMF over NMF are: 1) it could extract more useful features hidden in the original data through some kernel-induced nonlinear mappings; 2) it can deal with data where only relationships (similarities or dissimilarities) between objects are known; 3) it can process data with negative values by using some specific kernel functions (e.g. Gaussian). Thus, KNMF is more general than NMF. To further improve the performance of KNMF, we also propose the SpKNMF, which performs KNMF on sub-patterns of the original data. The effectiveness of the proposed algorithms is validated by extensive experiments on UCI datasets and the FERET face database.

Title: Global---local fisher discriminant approach for face recognition
Abstract: In this paper, we proposed a linear discriminant approach, namely global---local Fisher discriminant analysis (GLFDA) that explicitly considers both the local and global discriminant structures embedded in data. To be specific, GLFDA constructs two graphs to, respectively, model the global and local discriminant structures and then incorporates discriminant structures and local intrinsic structure, which characterizes the within-class compactness, into the objective function for dimensionality reduction. Thus, GLFDA well encodes the discriminant information, especially the local discriminant information of data. Experimental results on AR, YALE, and UMIST databases show the effectiveness of the proposed algorithm.

Title: Sparse regularization discriminant analysis for face recognition
Abstract: Recently the underlying sparse representation structure in high dimensional data has attracted considerable interests in pattern recognition and computer vision. Sparse representation structure means the sparse reconstructive relationship of the data. In this paper, we propose a novel dimensionality reduction method called Sparse Regularization Discriminant Analysis (SRDA), which aims to preserve the sparse representation structure of the data when learning an efficient discriminating subspace. More specifically, SRDA first constructs a concatenated dictionary through class-wise PCA decompositions which conduct PCA on data from each class separately, and learns the sparse representation structure under the constructed dictionary quickly through matrix-vector multiplications. Then SRDA takes into account both the sparse representation structure and the discriminating efficiency by using the learned sparse representation structure as a regularization term of linear discriminant analysis. Finally, the optimal embedding of the data is learned via solving a generalized eigenvalue problem. The extensive and promising experimental results on four publicly available face data sets (Yale, Extended Yale B, ORL and CMU PIE) validate the feasibility and effectiveness of the proposed method.

Title: Face recognition using Elasticfaces
Abstract: Though principle component analysis (PCA) and locality preserving projections (LPPs) are two of the most popular linear methods for face recognition, PCA can only see the Euclidean structure of the training set and LPP preserves the nonlinear submanifold structure hidden in the training set. In this paper, we propose the elastic preserving projections (EPPs) which by incorporating the merits of the local geometry and the global information of the training set. EPP outputs a sample subspace which simultaneously preserves the local geometrical structure and exploits the global information of the training set. Different from some other linear dimensionality reduction methods, EPP can be deemed as learning both the coordinates and the affinities between sample points. Furthermore, the effectiveness of our proposed algorithm is analyzed theoretically and confirmed by some experiments on several well-known face databases. The obtained results indicate that EPP significantly outperforms its other rival algorithms.

Title: Class-specific discriminant non-negative matrix factorization for frontal face verification
Abstract: In this paper, a supervised feature extraction method having both non-negative bases and weights is proposed. The idea is to extend the Non-negative Matrix Factorization (NMF) algorithm in order to extract features that enforce not only the spatial locality, but also the separability between classes in a discriminant manner. The proposed method incorporates discriminant constraints inside the NMF decomposition in a class specific manner. Thus, a decomposition of a face to its discriminant parts is obtained and new update rules for both the weights and the basis images are derived. The introduced methods have been applied to the problem of frontal face verification using the well known XM2VTS database. The proposed algorithm greatly enhance the performance of NMF for frontal face verification.

Title: Nonlinear nonnegative matrix factorization based on Mercer kernel construction
Abstract: Generalizations ofnonnegative matrix factorization (NMF) in kernel feature space, such as projected gradient kernel NMF (PGKNMF) and polynomial Kernel NMF (PNMF), have been developed for face and facial expression recognition recently. However, these existing kernel NMF approaches cannot guarantee the nonnegativity of bases in kernel feature space and thus are essentially semi-NMF methods. In this paper, we show that nonlinear semi-NMF cannot extract the localized components which offer important information in object recognition. Therefore, nonlinear NMF rather than semi-NMF is needed to be developed for extracting localized component as well as learning the nonlinear structure. In order to address the nonlinear problem of NMF and the semi-nonnegative problem of the existing kernel NMF methods, we develop the nonlinear NMF based on a self-constructed Mercer kernel which preserves the nonnegative constraints on both bases and coefficients in kernel feature space. Experimental results in face and expressing recognition show that the proposed approach outperforms the existing state-of-the-art kernel methods, such as KPCA, GDA, PNMF and PGKNMF.

Title: Local and Non-Local Feature-Based Kernel Nonnegative Matrix Factorization Method for Face Recognition
Abstract: Based on the kernel method and graph theory, this paper proposes a novel Kernel Non-negative Matrix Factorization with Local and Non-local feature (LN-KNMF) approach for face recognition. We establish the objective function in kernel space which incorporates two scatter quantities, namely local scatter and non-local scatter. They are determined by the local adjacent graph matrix and non-local adjacent graph matrix respectively. The update rules of the proposed LN-KNMF method are derived using polynomial kernel function and gradient descent method. Subsequently, we theoretically prove the convergence of the proposed algorithm by means of a constructed auxiliary function. Experimental results on the ORL face database demonstrate the superior performance of our approach against some NMF-based methods.

Title: Concept Factorization With Adaptive Neighbors for Document Clustering
Abstract: In this paper, a novel concept factorization (CF) method, called CF with adaptive neighbors (CFANs), is proposed. The idea of CFAN is to integrate an ANs regularization constraint into the CF decomposition. The goal of CFAN is to extract the representation space that maintains geometrical neighborhood structure of the data. Similar to the existing graph-regularized CF, CFAN builds a neighbor graph weights matrix. The key difference is that the CFAN performs dimensionality reduction and finds the neighbor graph weights matrix simultaneously. An efficient algorithm is also derived to solve the proposed problem. We apply the proposed method to the problem of document clustering on the 20 Newsgroups, Reuters-21578, and TDT2 document data sets. Our experiments demonstrate the effectiveness of the method.

Title: Joint Sparse Representation and Embedding Propagation Learning: A Framework for Graph-Based Semisupervised Learning
Abstract: In this paper, we propose a novel graph-based semisupervised learning framework, called joint sparse representation and embedding propagation learning (JSREPL). The idea of JSREPL is to join EPL with sparse representation to perform label propagation. Like most of graph-based semisupervised propagation learning algorithms, JSREPL also constructs weights graph matrix from given data. Different from classical approaches which build weights graph matrix and estimate the labels of unlabeled data in sequence, JSREPL simultaneously builds weights graph matrix and estimates the labels of unlabeled data. We also propose an efficient algorithm to solve the proposed problem. The proposed method is applied to the problem of semisupervised image clustering using the ORL, Yale, PIE, and YaleB data sets. Our experiments demonstrate the effectiveness of our proposed algorithm.

Title: Sparsity preserving projections with applications to face recognition
Abstract: Dimensionality reduction methods (DRs) have commonly been used as a principled way to understand the high-dimensional data such as face images. In this paper, we propose a new unsupervised DR method called sparsity preserving projections (SPP). Unlike many existing techniques such as local preserving projection (LPP) and neighborhood preserving embedding (NPE), where local neighborhood information is preserved during the DR procedure, SPP aims to preserve the sparse reconstructive relationship of the data, which is achieved by minimizing a L1 regularization-related objective function. The obtained projections are invariant to rotations, rescalings and translations of the data, and more importantly, they contain natural discriminating information even if no class labels are provided. Moreover, SPP chooses its neighborhood automatically and hence can be more conveniently used in practice compared to LPP and NPE. The feasibility and effectiveness of the proposed method is verified on three popular face databases (Yale, AR and Extended Yale B) with promising results.

Title: Laplacian Discriminant Projection with Optimized Kernels for Supervised Feature Extraction and Classification
Abstract: A novel feature extraction method, namely Laplacian discriminant projection with optimized kernels (KLDP-Opt) algorithm is proposed in this paper. The advantage of KLDP-Opt lies in: 1) the similarity matrix is constructed with the class-wise nonparametric similarity measure where it solves procedure selection problem; 2) data-dependent kernel is applied to solve the limitation of linearity of LPP, where the adaptive parameters of the data-dependent kernel are computed through optimizing an objective function designed for measuring the class separability of data in the feature space. Besides the theory derivation, the experiments are implemented on ORL and Yale face databases to evaluate the feasibility of the proposed algorithm.

Title: Graph-Regularized Local Coordinate Concept Factorization for Image Representation
Abstract: Existing matrix factorization based techniques, such as nonnegative matrix factorization and concept factorization, have been widely applied for data representation. In order to make the obtained concepts to be as close to the original data points as possible, one state-of-the-art method called locality constraint concept factorization is put forward, which represent the data by a linear combination of only a few nearby basis concepts. But its locality constraint does not well reveal the intrinsic data structure since it only requires the concept to be as close to the original data points as possible. To address these problems, by considering the manifold geometrical structure in local concept factorization via graph-based learning, we propose a novel algorithm, called graph-regularized local coordinate concept factorization (GRLCF). By constructing a parameter-free graph using constrained Laplacian rank (CLR) algorithm, we also present an extension of GRLCF algorithm as \(\hbox {GRLCF}_{\mathrm{CLR}}\). Moreover, we develop the iterative updating optimization schemes, and provide the convergence proof of our optimization scheme. Since GRLCF simultaneously considers the geometric structures of the data manifold and the locality conditions as additional constraints, it can obtain more compact and better structured data representation. Experimental results on ORL, Yale and Mnist image datasets demonstrate the effectiveness of our proposed algorithm.

Title: Image recognition with LPP mixtures
Abstract: Locality preserving projections (LPP) can find an embedding that preserves local information and discriminates data well. However, only one projection matrix over the whole data is not enough to discriminate complex data. In this paper, we proposed locality preserving projections mixture models (LPP mixtures), where the set of all data were partitioned into several clusters and a projection matrix for each cluster was obtained. In each cluster, We performed LPP via QR-decomposition, which is efficient computationally in under-sampled situations. Its theoretical foundation was presented. Experiments on a synthetic data set and the Yale face database showed the superiority of LPP mixtures.

Title: Nonlinear Nonnegative Matrix Factorization Based on Discriminant Analysis with Application to Face Recognition
Abstract: Traditional Nonnegative Matrix Factorization (NMF) is a linear and unsupervised algorithm. This would limit the classification power of NMF for the complicated data. To overcome the above limitations of NMF, this paper proposes a novel supervised and nonlinear NMF algorithm based on kernel theory and discriminant analysis. We incorporate the class label information into the decomposition of NMF in the Reproducing Kernel Hilbert Space (RKHS). A new iterative algorithm for NMF is derived and the objective function is non-increasing under the update rules. The proposed method is evaluated on the ORL and Yale face databases. The experimental results demonstrate the the proposed method is superior to the state of-the-art algorithms.

Title: Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification
Abstract: In this paper, two supervised methods for enhancing the classification accuracy of the Nonnegative Matrix Factorization (NMF) algorithm are presented. The idea is to extend the NMF algorithm in order to extract features that enforce not only the spatial locality, but also the separability between classes in a discriminant manner. The first method employs discriminant analysis in the features derived from NMF. In this way, a two-phase discriminant feature extraction procedure is implemented, namely NMF plus Linear Discriminant Analysis (LDA). The second method incorporates the discriminant constraints inside the NMF decomposition. Thus, a decomposition of a face to its discriminant parts is obtained and new update rules for both the weights and the basis images are derived. The introduced methods have been applied to the problem of frontal face verification using the well-known XM2VTS database. Both methods greatly enhance the performance of NMF for frontal face verification

Title: Locality preserving discriminant projections for face and palmprint recognition
Abstract: A new subspace learning algorithm called locality preserving discriminant projections (LPDP) is proposed by adding the criterion of maximum margin criterion (MMC) into the objective function of locality preserving projections (LPP). LPDP retains the locality preserving characteristic of LPP and utilizes the global discriminative structures obtained from MMC, which can maximize the between-class distance and minimize the within-class distance. Thus, our proposed LPDP combining manifold criterion and Fisher criterion has more discriminanting power, and is more suitable for recognition tasks than LPP, which considers only the local information for classification tasks. Moreover, two kinds of tensorized (multilinear) forms of LPDP are also derived in this paper. One is iterative while the other is non-iterative. The proposed LPDP method is applied to face and palmprint biometrics and is examined using the Yale and ORL face image databases, as well as the PolyU palmprint database. Experimental results demonstrate the effectiveness of the proposed LPDP method.

Title: Constrained Projective Non-negative Matrix Factorization for Semi-supervised Multi-label Learning
Abstract: This paper formulates multi-label learning as a constrained projective non-negative matrix factorization (CPNMF) problem which concentrates on a variant of the original projective NMF (PNMF) and explicitly introduces an auxiliary basis to learn the semantic subspace and boosts its discriminating ability by exploiting labeled and unlabeled examples together. Particularly, it propagates labels of the labeled examples to the unlabeled ones by enforcing coefficients of examples sharing identical semantic contents to be identical based on a hard constraint, i.e., embedding the class indicator of labeled examples into their coefficients. CPNMF preserves the geometrical structure of dataset via manifold regularization meanwhile captures the inherent structure of labels by using label correlations. We developed a multiplicative update rule (MUR) based algorithm to optimize CPNMF and proved its convergence. Experiments of image annotation on Corel dataset, text categorization on Rcv1v2 dataset, and text clustering on two popular text corpuses suggest the effectiveness of CPNMF.

Title: Structure Constrained Discriminative Non-negative Matrix Factorization for Feature Extraction
Abstract: In this paper, we propose a novel algorithm called Structure Constrained Discriminative Non-negative Matrix Factorization (SCDNMF) for feature extraction. In our proposed algorithm, a pixel dispersion penalty (PDP) constraint is employed to preserve spatial locality structured information of the basis obtained by NMF. At the same time, in order to improve the classification performance, intra-class graph and inter-class graph are also constructed to exploit discriminative information as well as geometric structure of the highdimensional data. Therefore, the low-dimensional features obtained by our algorithm are structured sparse and discriminative. Moreover, an iterative updating optimization scheme is developed to solve the objective function of the proposed SCDNMF. The proposed method is applied to the problem of image recognition using the well-known ORL, Yale and COIL20 databases. The experimental results demonstrate that the performance of our proposed SCDNMF outperforms the state-of-the-art methods.

Title: Fuzzy maximal marginal embedding and its application
Abstract: In this paper, we develops a new approach, called fuzzy maximal marginal embedding (FMME), combining LMME (local maximal marginal embedding) with fuzzy set theory, in which the fuzzy k-nearest neighbor (FKNN) is implemented to achieve the nature distribution information of original samples, and this information is utilized to redefine the affinity weights of neighborhood graph (intraclass and interclass ) instead of the weights of the binary pattern. We can reduce sensitivity of the method to substantial variations between samples caused by varying illumination and shape, viewing conditions. That makes FMME more powerful and robust than other method. The proposed algorithm is examined using Yale and ORL face image databases. The experimental results show FMME outperforms PCA, LDA, LPP and LMME.

Title: Discriminant non-negative graph embedding for face recognition
Abstract: Abstract   Non-negative Matrix Factorization (NMF) is an unsupervised algorithm for low-rank approximation of non-negative data and has been widely used in many fields, but its performance in feature extraction is not satisfactory. The main reason is that the model of NMF and its variants did not take into account the label information of the samples, which can add the discriminant ability of the methods. In this paper, we proposed a novel method, called discriminant non-negative graph embedding (DNGE) algorithm in which the label information of the samples and the local geometric structure are all integrated in the objective function. Furthermore, we incorporated the between-class graph and within-class graph into the objective functions to indicate that we not only used the local separability but also used the whole separability of the samples. To guarantee convergence, we use the KKT condition to calculate the non-negative solution of the DNGE. A convergent multiplicative non-negative updating rule is then derived to learn the transformation matrix. Experiments are conducted on the CMU PIE, ORL, Yale, FERET and AR database. The results show that the DNGE algorithm provides better facial representation and achieves higher recognition rates than naive Non-Negative Matrix Factorization and its extension methods.

Title: Constrained concept factorization with graph Laplacian
Abstract: Concept Factorization (CF) is a modified version of Nonnegative Matrix Factorization (NMF) and both of them have been proved to be effective matrix factorization methods for dimensionality reduction and data clustering. However, CF is essentially an unsupervised method which cannot utilize any prior knowledge of data. In this paper, we propose a new semi-supervised concept factorization method, called Constrained Concept Factorization with Graph Laplacian (CCF-GL), which not only incorporates the geometrical information of data, but also utilizes the prior label information to enhance the accuracy of CF. Specifically, we expect that the graph laplacian could preserve the intrinsic manifold structure of original data. Meanwhile, in the low-dimensional space, we hope that data points sharing the same label will have the same coordinate, while the coordinates of data points possessing different labels will be as dissimilar as possible. As a result, the learning quality of this semi-supervised CF method has been significantly enhanced. The experimental results on image clustering show good performance of our algorithm.

Title: Graph-regularized CF with local coordinate for image representation
Abstract: Abstract   Concept factorization (CF) has been a powerful data representation method, which has been widely applied in image processing and document clustering. However, traditional CF cannot guarantee the decomposition results of CF to be sparse in theory and do not consider the geometric structure of the databases. In this paper, we propose a graph-regularized CF with local coordinate (LGCF) method, which enforces the learned coefficients to be sparse by using the local coordinate constraint meanwhile preserving the intrinsic geometric structure of the data space by incorporating graph regularization. An iterative optimization method is also proposed to solve the objective function of LGCF. By comparing with the state-of-the-arts algorithms (Kmeans, NMF, CF, LCCF, LCF), experimental results on four popular databases show that the proposed LGCF method has better performance in terms of average accuracy and mutual information.

Title: Graph Regularized Non-negative Matrix with L0-Constraints for Selecting Characteristic Genes
Abstract: Non-negative Matrix Factorization (NMF) has been widely concerned in computer vision and data representation. However, the penalized and restriction L0-norm measure are imposed on the NMF model in traditional NMF methods. In this paper, we propose a novel graph regularized non-negative matrix with L0-constraints (GL0NMF) method which comprises the geometrical structure and a more interpretation sparseness measure. In order to extract the characteristic gene effectively, the steps are shown as follows. Firstly, the original data \( {\mathbf{Q}} \) is decomposed into two non-negative matrices \( {\mathbf{F}} \) and \( {\mathbf{P}} \) by utilizing GL0NMF method. Secondly, characteristic genes are extracted by the sparse matrix \( {\mathbf{F}} \). Finally, the extracted characteristic genes are validated by using Gene Ontology. In conclusion, the results demonstrate that our method can extract more genes than other conventional gene selection methods.

Title: Hidden space discriminant neighborhood embedding
Abstract: Discriminant neighborhood embedding (DNE) algorithm is one of supervised linear dimensionality reduction methods. Its nonlinear version kernel discriminant neighborhood embedding (KDNE) is expected to behave well on classification tasks. However, since KDNE constructs an adjacent graph in the original space, the adjacency graph could not represent the adjacent information in the kernel mapping space. By introducing hidden space, this paper proposes a novel nonlinear method for DNE, called hidden space discriminant neighborhood embedding (HDNE). This algorithm first maps the data in the original space into a high dimensional hidden space by a set of nonlinear hidden functions, and then builds an adjacent graph incorporating neighborhood information of the dataset in the hidden space. Finally, DNE is used to find a transformation matrix which would map the data in the hidden space to a low-dimensional subspace. The proposed method is applied to ORL face and MNIST handwritten digit databases. Experimental results show that the proposed method is efficiency for classification tasks.

Title: Tensor Graph-optimized Linear Discriminant Analysis
Abstract: Graph-based Fisher Analysis (GbFA) is proposed recently for dimensionality reduction, which has the powerful discriminant ability. However, GbFA is based on the matrix-to-vector way, which not only costs much but also loses spatial relations of pixels in images. Therefore, Tensor Graph-based Linear Discriminant Analysis (TGbLDA) is proposed in the paper. TGbLDA regards samples as data in tensor space and gets projection matrixes through the iteration way. Besides, TGbLDA inherits merits of GbFA. Experiments on Yale and YaleB face datasets demonstrate the effectiveness of our proposed algorithm.

Title: Global Versus Local Methods in Nonlinear Dimensionality Reduction
Abstract: Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages: global (Isomap [1]), and local (Locally Linear Embedding [2], Laplacian Eigenmaps [3]). We present two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps.

Title: Linear discriminant projection embedding based on patches alignment
Abstract: Dimensionality reduction is often required as a preliminary stage in many data analysis applications. In this paper, we propose a novel supervised dimensionality reduction method, called linear discriminant projection embedding (LDPE), for pattern recognition. LDPE first chooses a set of overlapping patches which cover all data points using a minimum set cover algorithm with geodesic distance constraint. Then, principal component analysis (PCA) is applied on each patch to obtain the data's local representations. Finally, patches alignment technique combined with modified maximum margin criterion (MMC) is used to yield the discriminant global embedding. LDPE takes both label information and structure of manifold into account, thus it can maximize the dissimilarities between different classes and preserve data's intrinsic structures simultaneously. The efficiency of the proposed algorithm is demonstrated by extensive experiments using three standard face databases (ORL, YALE and CMU PIE). Experimental results show that LDPE outperforms other classical and state of art algorithms.

Title: A global manifold margin learning method for data feature extraction and classification
Abstract: Abstract   This paper presents a global manifold margin learning approach for data feature extraction or dimensionality reduction, which is named locally linear representation manifold margin (LLRMM). Provided that points locating on one manifold are of the same class and those residing on the corresponding manifolds are varied labeled, LLRMM is desired to identify different manifolds, respectively. In the proposed LLRMM, it firstly constructs both a between-manifold graph and a within-manifold graph. In the between-manifold graph, for any point, its k nearest neighbors and itself must belong to different manifolds. However, any node and its neighborhood points should be on the same manifold in the within-manifold graph. Then we use the minimum locally linear representation trick to reconstruct any node with their corresponding k nearest neighbors in both graphs, from which a between-manifold graph scatter and a within-manifold graph scatter can be reasoned, followed by a novel global model of manifold margin. At last, a projection will be explored to map the original data into a low dimensional subspace with the maximum manifold margin. Experiments on some widely used face data sets including AR, CMU PIE, Yale, YaleB and LFW have been carried out, where the performance of the proposed LLRMM outperforms those of some other methods such as kernel principal component analysis (KPCA), non-parametric discriminant analysis (NDA), reconstructive discriminant analysis (RDA), discriminant multiple manifold learning (DMML) and large margin nearest neighbor (LMNN).

Title: Virtual Label Constraint Nonnegative Matrix Factorization
Abstract: This paper proposes a novel Semi-supervised Nonnegative Matrix Factorization (NMF), called Virtual Label Constraint Nonnegative Matrix Factorization (VLCNMF). The idea of the VLCNMF is to extend the NMF by incorporating a virtual label constraint into the NMF decomposition. Different from previous works, our approach covers two main steps: the first step is to obtain virtual labels by label propagation algorithms and the second step is to add these virtual labels information as additional constraints into original NMF. The proposed VLCNMF approach is applied to the problem of semi-supervised image representation using the well-known ORL, Yale datasets.

Title: Image Feature Extraction via Graph Embedding Regularized Projective Non-negative Matrix Factorization
Abstract: Non-negative matrix factorization (NMF) has been widely used in image processing and pattern recognition fields. Unfortunately, NMF does not consider the geometrical structure and the discriminative information of data, which might make it unsuitable for classification tasks. In addition, NMF only calculates the coefficient matrix of the training data and how to yields the coefficient vector of a new test data is still obscure. In this paper, we propose a novel graph embedding regularized projective non-negative matrix factorization (GEPNMF) method to address the aforementioned problems. By introducing a graph embedding regularization term, the learned subspace can preserve the local geometrical structure of data while maximizing the margins of different classes. We deduce a multiplicative update rule (MUR) to iteratively solve the objective function of GEPNMF and prove its convergence in theory. Experimental results on ORL and CMU PIE databases suggest the effectiveness of GEPNMF.

Title: Local Discriminative Orthogonal Rank-One Tensor Projection for image feature extraction
Abstract: This paper develops a Local Discriminative Orthogonal Rank-One Tensor Projection (LDOROTP) technique for image feature extraction. The goal of LDOROTP is to learn a compact feature for images meanwhile endow the feature with prominent discriminative ability. LDOROTP achieves the goal through a serial of rank-one tensor projections with orthogonal constraints. To seek the optimal projections, LDOROTP carries out local discriminant analysis, but differs from the previous works on two aspects: (1)the local neighborhood consists of all the samples of the same class and partial local samples from different classes; (2)a novel weighting function is designed to encode the local discriminant information. The criterion of LDOROTP is built on the trace differences of matrices rather than the trace ratio, so the awkward problem of singular matrix do not emerges. Besides, LDOROTP benefits from an efficient and stable iterative scheme of solution and a data preprocessing called GLOCAL tensor representation. LDOROTP is evaluated on face recognition application on two benchmark databases: Yale and PIE, and compared with several popular projection techniques. Experimental results suggest that the proposed LDOROTP provides a supervised image feature extraction approach of powerful pattern revealing capability.

Title: Face recognition using neighborhood preserving projections
Abstract: Subspace learning is one of the main directions for face recognition. In this paper, a novel unsupervised subspace learning method, Neighborhood Preserving Projections (NPP), is proposed. In contrast to traditional linear dimension reduction method, such as principal component analysis (PCA), the proposed method has good neighborhood-preserving property. The central idea is to modify the classical locally linear embedding by introducing a linear transform matrix. The transform matrix is obtained by optimizing a certain objective function. Experimental results on Yale face database and FERET face database show the effectiveness of the proposed method....

Title: Face recognition using neighborhood preserving projections
Abstract: Subspace learning is one of the main directions for face recognition. In this paper, a novel unsupervised subspace learning method, Neighborhood Preserving Projections (NPP), is proposed. In contrast to traditional linear dimension reduction method, such as principal component analysis (PCA), the proposed method has good neighborhood-preserving property. The central idea is to modify the classical locally linear embedding by introducing a linear transform matrix. The transform matrix is obtained by optimizing a certain objective function. Experimental results on Yale face database and FERET face database show the effectiveness of the proposed method....

Title: Semi-supervised local ridge regression for local matching based face recognition
Abstract: In this paper, a novel algorithm named Semi-supervised Local Ridge Regression (SSLRR) is proposed for local matching based face recognition. Compared with other algorithms, the proposed algorithm possesses two advantages. Firstly, SSLRR utilizes a multiple graph based semi-supervised technique to propagate the class labels of labeled samples to the unlabeled ones. Thus, the information of both labeled and unlabeled data can be employed in our algorithm to improve its performance. Secondly, unlike most local matching based face recognition algorithms which assume different sub-images from the same face are independent, an adaptive non-negative weight vector is introduced into our SSLRR to combine the Laplacian matrices obtained by different sub-images. Therefore, the latent complementary information of multiple sub-patterns from the same face image can be taken into account. Moreover, a simple yet efficient iterative update scheme is also proposed to solve our SSLRR model. Extensive experiments are performed on five standard face databases (Yale, Extended YaleB, AR, CMU PIE and LFW) to demonstrate the efficiency of the proposed algorithm. Experimental results show that SSLRR obtains better recognition performance than some other state-of-the-art approaches.

Title: LDA-based Non-negative Matrix Factorization for Supervised Face Recognition
Abstract: In PCA based face recognition, the basis imagesmay contain negative pixels and thus do not facilitatephysical interpretation. Recently, the technique of nonnegativematrix Factorization (NMF) has been applied to face recognition: the non-negativity constraint of NMF yieldsa localized parts-based representation which achieves arecognition rate that is on par with the eigenface approach.    In this paper, we propose a new variation of the NMF algorithm that incorporates training information in a supervised learning setting. We integrate an additional term basedon Fisher’s Linear Discriminant Analysis into the NMF algorithm and prove that our new update rule can maintainthe non-negativity constraint under a mild condition andhence preserve the intuitive meaning for the base vectors and weight vectors while facilitating the supervised learning of within-class and between-class information.    We tested our new algorithm on the well-known ORL database, CMU PIE database and FERET database, and the results from experiments are very encouraging compared with traditional techniques including the original NMF, the Eigenface method, the sequential NMF+LDA method and the Fisherface method.

Title: Uncorrelated slow feature discriminant analysis using globality preserving projections for feature extraction
Abstract: Slow Feature Discriminant Analysis (SFDA) is a supervised feature extraction method for classification inspired by biological mechanism. However, SFDA only considers the local geometrical structure information of data and ignores the global geometrical structure information. Furthermore, previous works have demonstrated that uncorrelated features of minimum redundancy are effective for classification. In this paper, a novel method called uncorrelated slow feature discriminant analysis using globality preserving projections (USFDA-GP) is proposed for feature extraction and recognition. In USFDA-GP, two kinds of global information are imposed to the objective function of conventional SFDA for respecting some more global geometric structures. We also provide an analytical solution by simple eigenvalue decomposition to the optimal model instead of previous iterative method. Experimental results on Extended YaleB, CMU PIE and LFW-a face databases demonstrate the effectiveness of our proposed method.

Title: Two-dimensional local graph embedding discriminant analysis (2DLGEDA) with its application to face and palm biometrics
Abstract: This paper proposes a novel method, called two-dimensional local graph embedding discriminant analysis (2DLGEDA), for image feature extraction, which can directly extract the optimal projective vectors from two-dimensional image matrices rather than image vectors based on the scatter difference criterion. In graph embedding, the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring within the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. The proposed method effectively avoids the singularity problem frequently encountered in the traditional linear discriminant analysis algorithm (LDA) due to the small sample size (SSS) and overcomes the limitations of LDA due to data distribution assumptions and available projection directions. Experimental results on ORL, YALE, FERET face databases and PolyU palmprint database show the effectiveness of the proposed method.

Title: Block-wise 2D kernel PCA/LDA for face recognition
Abstract: Direct extension of (2D) matrix-based linear subspace algorithms to kernel-induced feature space is computationally intractable and also fails to exploit local characteristics of input data. In this letter, we develop a 2D generalized framework which integrates the concept of kernel machines with 2D principal component analysis (PCA) and 2D linear discriminant analysis (LDA). In order to remedy the mentioned drawbacks, we propose a block-wise approach based on the assumption that data is multi-modally distributed in so-called block manifolds. Proposed methods, namely block-wise 2D kernel PCA (B2D-KPCA) and block-wise 2D generalized discriminant analysis (B2D-GDA), attempt to find local nonlinear subspace projections in each block manifold or alternatively search for linear subspace projections in kernel space associated with each blockset. Experimental results on ORL face database attests to the reliability of the proposed block-wise approach compared with related published methods.

Title: Constrained Non-negative Matrix Factorization with Graph Laplacian
Abstract: Non-negative Matrix Factorization (NMF) is proven to be a very effective decomposition method for dimensionality reduction in data analysis, and has been widely applied in computer vision, pattern recognition and information retrieval. However, NMF is virtually an unsupervised method since it is unable to utilize prior knowledge about data. In this paper, we present Constrained Non-negative Matrix Factorization with Graph Laplacian (CNMF-GL), which not only employs the geometrical information, but also properly uses the label information to enhance NMF. Specifically, we expect that a graph regularized term could preserve the local structure of original data, meanwhile data points both having the same label and possessing different labels will have corresponding constraint conditions. As a result, the learned representations will have more discriminating power. The experimental results on image clustering manifest the effectiveness of our algorithm.

Title: Multilinear Spatial Discriminant Analysis for Dimensionality Reduction
Abstract: In the last few years, great efforts have been made to extend the linear projection technique (LPT) for multidimensional data (i.e., tensor), generally referred to as the multilinear projection technique (MPT). The vectorized nature of LPT requires high-dimensional data to be converted into vector, and hence may lose spatial neighborhood information of raw data. MPT well addresses this problem by encoding multidimensional data as general tensors of a second or even higher order. In this paper, we propose a novel multilinear projection technique, called multilinear spatial discriminant analysis (MSDA), to identify the underlying manifold of high-order tensor data. MSDA considers both the nonlocal structure and the local structure of data in the transform domain, seeking to learn the projection matrices from all directions of tensor data that simultaneously maximize the nonlocal structure and minimize the local structure. Different from multilinear principal component analysis (MPCA) that aims to preserve the global structure and tensor locality preserving projection (TLPP) that is in favor of preserving the local structure, MSDA seeks a tradeoff between the nonlocal (global) and local structures so as to drive its discriminant information from the range of the non-local structure and the range of the local structure. This spatial discriminant characteristic makes MSDA have more powerful manifold preserving ability than TLPP and MPCA. Theoretical analysis shows that traditional MPTs, such as multilinear linear discriminant analysis, TLPP, MPCA, and tensor maximum margin criterion, could be derived from the MSDA model by setting different graphs and constraints. Extensive experiments on face databases (ORL, CMU PIE, and the extended Yale-B) and the Weizmann action database demonstrate the effectiveness of the proposed MSDA method.

Title: Kernel Fisher NPE for Face Recognition
Abstract: Neighborhood Preserving Embedding (NPE) is a subspace learning algorithm. Since NPE is a linear approximation to Locally Linear Embedding (LLE) algorithm, it has good neighborhood-preserving properties. Although NPE has been applied in many fields, it has limitations to solve recognition task. In this paper, a novel subspace method, named Kernel Fisher Neighborhood Preserving Embedding (KFNPE), is proposed. In this method, discriminant information as well as the intrinsic geometry relations of the local neighborhoods are preserved according to prior class-label information. Moreover, complex nonlinear variations of real face images are represented by nonlinear kernel mapping. Experimental results on ORL face database demonstrate the effectiveness of the proposed method.

Title: Enhanced semi-supervised local Fisher discriminant analysis for face recognition
Abstract: An improved manifold learning method, called enhanced semi-supervised local Fisher discriminant analysis (ESELF), for face recognition is proposed. Motivated by the fact that statistically uncorrelated and parameter-free are two desirable and promising characteristics for dimension reduction, a new difference-based optimization objective function with unlabeled samples has been designed. The proposed method preserves the manifold structure of labeled and unlabeled samples in addition to separating labeled samples in different classes from each other. The semi-supervised method has an analytic form of the globally optimal solution and it can be computed based on eigen decomposition. Experiments on synthetic data and AT&T, Yale and CMU PIE face databases are performed to test and evaluate the proposed algorithm. The experimental results and comparisons demonstrate the effectiveness of the proposed method.

Title: Sparse projections over graph
Abstract: Recent study has shown that canonical algorithms such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) can be obtained from graph based dimensionality reduction framework. However, these algorithms yield projective maps which are linear combination of all the original features. The results are difficult to be interpreted psychologically and physiologically. This paper presents a novel technique for learning a sparse projection over graphs. The data in the reduced subspace is represented as a linear combination of a subset of the most relevant features. Comparing to PCA and LDA, the results obtained by sparse projection are often easier to be interpreted. Our algorithm is based on a graph embedding model, which encodes the discriminating and geometrical structure in terms of the data affinity. Once the embedding results are obtained, we then apply regularized regression for learning a set of sparse basis functions. Specifically, by using L1-norm regularizer (e.g. lasso), the sparse projections can be efficiently computed. Experimental results on two document databases demonstrate the effectiveness of our method.

Title: Collaborative representation based local discriminant projection for feature extraction
Abstract: Abstract   This paper introduces a novel dimensionality reduction algorithm, called collaborative representation based local discriminant projection (CRLDP), for feature extraction. CRLDP utilizes collaborative representation relationships among samples to construct adjacency graphs. Different from most graph-based algorithms which manually construct the adjacency graphs, CRLDP is able to automatically construct the graphs and avoid manually choosing nearest neighbors. In CRLDP, two graphs (the within-class graph and the between-class graph) are constructed. Based on the two constructed graphs, the within-class scatter and the between-class scatter are computed to characterize the compactness and separability of samples, respectively. Then CRLDP seeks to find an optimal projection matrix to maximize the ratio of the between-class scatter to the within-class scatter. Experimental results on ORL, AR and CMU PIE face databases validate the superiority of CRLDP over other state-of-the-art algorithms.

Title: Feature extraction by learning Lorentzian metric tensor and its extensions
Abstract: We develop a supervised dimensionality reduction method, called Lorentzian discriminant projection (LDP), for feature extraction and classification. Our method represents the structures of sample data by a manifold, which is furnished with a Lorentzian metric tensor. Different from classic discriminant analysis techniques, LDP uses distances from points to their within-class neighbors and global geometric centroid to model a new manifold to detect the intrinsic local and global geometric structures of data set. In this way, both the geometry of a group of classes and global data structures can be learnt from the Lorentzian metric tensor. Thus discriminant analysis in the original sample space reduces to metric learning on a Lorentzian manifold. We also establish the kernel, tensor and regularization extensions of LDP in this paper. The experimental results on benchmark databases demonstrate the effectiveness of our proposed method and the corresponding extensions.

Title: Feature Selection for Adaptive Dual-Graph Regularized Concept Factorization for Data Representation
Abstract: Recently, manifold regularization with the affinity graph in matrix factorization-related studies, such as dual-graph regularized concept factorization (GCF), have yielded impressive results for clustering. However, due to the noisy and irrelevant features of the data samples, the affinity graph constructed directly from the original feature space is not necessarily a reliable reflection of the intrinsic manifold of the data samples. To overcome this problem, we integrate feature selection into the construction of the data (feature) graph and propose a novel algorithm called adaptive dual-graph regularized CF with Feature selection $$(\hbox {ADGCF}_{\mathrm{FS}})$$(ADGCFFS), which simultaneously considers the geometric structures of both the data manifold and the feature manifold. We unify feature selections, dual-graph regularized CF into a joint objective function and minimize this objective function with iterative and alternative updating optimization schemes. Moreover, we provide the convergence proof of our optimization scheme. Experimental results on TDT2 and Reuters document datasets, COIL20 and PIE image datasets demonstrate the effectiveness of our proposed method.

Title: Efficient algorithms for graph regularized PLSA for probabilistic topic modeling
Abstract: Abstract   Probabilistic latent semantic analysis (PLSA) is a popular data analysis method with the objective to discover the underlying semantic structure of input data. In this work, we describe a method for probabilistic topic analysis in image and text based on a new representation of graph-regularized PLSA (GPLSA). In GPLSA, data entities are mapped to an undirected graph, where similarities between topic compositions on the graph are measured by the divergence between discrete probabilities. Such divergence is essentially incorporated as a graph-regularizer that augments the original PLSA algorithm. Furthermore, we extend the GPLSA algorithms to multiple data modalities based on the connections between data entities of each modality. We propose efficient multiplicative iterative algorithms for GPLSA with three popular regularizers, namely l 1 , l 2  and symmetric KL divergences. In each case, we derive simple efficient numerical solutions that require only matrix arithmetic operations during the optimization. Experimental results demonstrate the efficacy of GPLSA over state-of-the-art methods.

Title: Semi-Supervised Dimensionality Reduction with Pairwise Constraints Using Graph Embedding for Face Analysis
Abstract: Following the intuition that the image variation of faces can be effectively modeled by low dimensional linear spaces, we propose a novel linear subspace learning method for face analysis in the framework of graph embedding model, called semi-supervised graph embedding (SGE). This algorithm builds an adjacency graph which can best respect the geometry structure inferred from the must-link pairwise constraints, which specify a pair of instances belong to the same class. The projections are obtained by preserving such a graph structure. Using the notion of graph Laplacian, SGE has a closed solution of an eigen-problem of some specific Laplacian matrix and therefore it is quite efficient. Experimental results on Yale standard face database demonstrate the effectiveness of our proposed algorithm.

Title: A Novel Feature Extraction Algorithm Based on Joint Learning
Abstract: n this paper, a novel feature extraction algorithm, called Joint Discriminant Sparse Neighborhood Preserving Embedding (JDSNPE), based on Discriminant Sparse Neigh- borhood Preserving Embedding (DSNPE) and joint learning is proposed. JDSNPE aims to get the row sparsity of the transformation matrix while preserving discriminant sparse neighborhood. Experimental results on Yale database demon- strate the effectiveness of the proposed algorithm compared to Sparse Neighborhood Preserving Embedding and DSNPE.

Title: Incremental general non-negative matrix factorization without dimension matching constraints
Abstract: Abstract   In this paper, we propose a General Non-negative Matrix Factorization based on the left Semi-Tensor Product (lGNMF) and the General Non-negative Matrix Factorization based on the right Semi-Tensor Product (rGNMF), which factorize an input non-negative matrix into two non-negative matrices of lower ranks based on gradient method. In particular, the proposed models are able to remove the dimension matching constraints required by conventional NMF models. Both theoretical derivation and experimental results show that the conventional NMF is a special case of the proposed lGNMF and rGNMF. We find the method for the best efficacy of the image restoration in lGNMF and rGNMF by experiments on baboon and lenna images. Moreover, inspired by the Incremental Non-negative Matrix Factorization (INMF), we propose the Incremental lGNMF (IlGNMF) and Incremental rGNMF (IrGNMF), We also conduct the experiments on JAFFE database and ORL database, and find that IlGNMF and IrGNMF realize saving storage space and reducing computation time in incremental facial training.

Title: Two-dimensional bilinear preserving projections for image feature extraction and classification
Abstract: Two-dimensional locality preserving projections (2DLPP) was recently proposed to extract features directly from image matrices based on locality preserving criterion. A significant drawback of 2DLPP is that it only works on one direction (left or right) to reduce the dimensionality of the image matrices and thus too many coefficients are needed for image representation in low-dimensional subspace. In this paper, we propose a novel method called two-dimensional bilinear preserving projections (2DBPP) for image feature extraction. We generalized the image-based (2D-based) feature extraction techniques into bilinear cases, in which 2DLPP is a special case of our proposed method. In order to obtain the bilinear projections, we proposed an iteration method by solving the corresponding generalized eigen-equations. Moreover, analyses show that 2DBPP has stronger locality preserving abilities than 2DLPP. By using the label information and defining different local neighborhood graphs, the proposed framework is further extended to supervised case. Experiments on three databases show that 2DBPP and its supervised extension are superior to some other image-based state-of-the-art techniques.

Title: Discriminative locality preserving dimensionality reduction based on must-link constraints
Abstract: Locality preserving manifold learning algorithms are seeking intrinsic manifold based on overlapping local geometry structure. Locality Preserving Projections (LPP) and Neighborhood Preserving Embedding (NPE) are two representative linear locality preserving manifold learning algorithms, which not only defined on training samples, but also can generalize to test samples. But they just take the local structure into consideration, ignoring some available prior information. Pairwise constraints are easier obtained supervised information compared with labels. In this paper, we proposed two discriminative locality preserving manifold learning algorithms, by incorporating must-link constraints into LPP and NPE to improve their discriminative ability. Experiments results on Yale and ORL face databases verified the effectiveness.

Title: Learning spatially localized, parts-based representation
Abstract: In this paper, we propose a novel method, called local non-negative matrix factorization (LNMF), for learning spatially localized, parts-based subspace representation of visual patterns. An objective function is defined to impose a localization constraint, in addition to the non-negativity constraint in the standard NMF. This gives a set of bases which not only allows a non-subtractive (part-based) representation of images but also manifests localized features. An algorithm is presented for the learning of such basic components. Experimental results are presented to compare LNMF with the NMF and PCA methods for face representation and recognition, which demonstrates advantages of LNMF.

Title: A semi-supervised approach for dimensionality reduction with distributional similarity
Abstract: Semi-supervised learning has recently received considerable attention in machine learning. In this paper, we propose a novel diffusion maps based semi-supervised algorithm for dimensionality reduction, visualization and data representation. Unlike previous work which uses only geometric information for similarity metric construction, a distributional similarity metric is introduced to modify the geometric relationship of samples. This metric is defined using the posterior probability over the labels of each sample, which is learned through the Expectation-Maximization (EM) algorithm. The Euclidean distance between points on the intrinsic manifold learned by our proposed method is equal to the label-dependent ''diffusion distance'', which is modified by the distributional similarity related metric, in the original space. Our algorithm preserves the local manifold structure in addition to separating samples in different classes, thus facilitates the classification. Encouraging experimental results on handwritten digits, Yale faces, UCI data sets and the Weizmann data set show that the algorithm can improve the classification accuracy significantly.

Title: Regularized Kernel Locality Preserving Discriminant Analysis for Face Recognition
Abstract: In this paper, a regularized kernel locality preserving discriminant analysis (RKLPDA) method is proposed for facial feature extraction and recognition. The proposed RKLPDA comes into the characteristic of LPDA that encodes both the geometrical and discriminant structure of the data manifold, and improves the classification ability for linear non-separable data by introducing kernel trick. Meanwhile, by regularizing the eigenvectors of the kernel locality preserving within-class scatter, RKLPDA utilizes all the discriminant information and eliminates the small sample size (SSS) problem. Experiments on ORL and FERET face databases are performed to test and evaluate the proposed algorithm. The results demonstrate the effectiveness of RKLPDA.

Title: PRESERVING LOCAL MANIFOLD IN NOISE DATA CLUSTERING
Abstract: Adaptive subspace clustering techniques, such as LDA Kmeans (henceforth LDAKM), try to perform clustering on a compact discriminative subspace. However, the subspace extracted by LDAKM may be inferior when encountering noise, so it is with the clustering results. In this paper, we generalize the LDAKM algorithm, and propose the Local Manifold Preserving LDAKM (henceforth LMP–LDAKM) approach, which considers both local manifold structure and discriminative information. A Laplacian similarity matrix is introduced in the subspace extraction subprocess for preserving local manifold information while retaining discrimination of latent clusters, and it is also utilized for the weighted Kmeans clustering subprocess in the subspace to eliminate the influence of distant noise data. Experimental results on both benchmark and artificial datasets indicate the effectiveness and noise-robustness of our method.

Title: Non-negative Sparse Representation Based on Block NMF for Face Recognition
Abstract: This paper attempts to utilize the basis images of block non-negative matrix factorization (BNMF) to serve as the sparse learning dictionary, which is more suitable for non-negative sparse representation (NSR) because they have non-negative compatibility. Based on BNMF-basis-image dictionary, the NSR features of query facial images can be learnt directly by solving l1-regularized least square problems. The NSR-feature based algorithm is then developed and successfully applied to face recognition. Subsequently, to further enhance the discriminant power of NSR method, this paper also proposes a feature fusion approach via combining NSR-feature with BNMF-feature. The proposed algorithms are tested on ORL and FERET face databases. Experimental results show that the proposed NSR+BNMF method greatly outperforms two single-feature based methods, namely NSR method and BNMF method.

Title: Low-Rank Matrix Factorization With Adaptive Graph Regularizer
Abstract: In this paper, we present a novel low-rank matrix factorization algorithm with adaptive graph regularizer (LMFAGR). We extend the recently proposed low-rank matrix with manifold regularization (MMF) method with an adaptive regularizer. Different from MMF, which constructs an affinity graph in advance, LMFAGR can simultaneously seek graph weight matrix and low-dimensional representations of data. That is, graph construction and low-rank matrix factorization are incorporated into a unified framework, which results in an automatically updated graph rather than a predefined one. The experimental results on some data sets demonstrate that the proposed algorithm outperforms the state-of-the-art low-rank matrix factorization methods.

Title: A novel null space-based kernel discriminant analysis for face recognition
Abstract: The symmetrical decomposition is a powerful method to extract features for image recognition. It reveals the significant discriminative information from the mirror image of symmetrical objects. In this paper, a novel null space kernel discriminant method based on the symmetrical method with a weighted fusion strategy is proposed for face recognition. It can effectively enhance the recognition performance and shares the advantages of Null-space, kernel and symmetrical methods. The experiment results on ORL database and FERET database demonstrate that the proposed method is effective and outperforms some existing subspace methods.

Title: Graph-based discriminative concept factorization for data representation
Abstract: Nonnegative Matrix Factorization (NMF) and Concept Factorization (CF) have been widely used for different purposes such as feature learning, dimensionality reduction and image clustering in data representation. However, CF is a variant of NMF, which is an unsupervised learning method without making use of the available label information to guide the clustering process. In this paper, we put forward a semi-supervised discriminative concept factorization (SDCF) method, which utilizes the limited label information of the data as a discriminative constraint. This constraint forces the representation of data points within the same class should be very close together or aligned on the same axis in the new representation. Furthermore, in order to utilize the local manifold regularization, we propose a novel semi-supervised graph-based discriminative concept factorization (GDCF) method, which incorporates the local manifold regularization and the label information of the data into the CF to improve the performance of CF. GDCF not only encodes the local geometrical structure of the data space by constructing K-nearest graph, but also takes into account the available label information. Thus, the discriminative abilities of data representations are enhanced in the clustering tasks. Experimental results on several databases expose the strength of our proposed SDCF and GDCF methods compared to the state-of-the-art methods.

Title: A Subspace Learning Based on a Rank Symmetric Relation for Fuzzy Kernel Discriminant Analysis
Abstract: Classification of nonlinear high-dimensional data is usually not amenable to standard pattern recognition techniques because of an underlying nonlinear small sample size conditions. To address the problem, a novel kernel fuzzy dual discriminant analysis learning based on a rank symmetric relation is developed in this paper. First, dual subspaces with rank symmetric relation on the discriminant analysis are established, by which a set of integrated subspaces of within-class and between-class scatter matrices are constructed, respectively. Second, a reformative fuzzy LDA algorithm is proposed to achieve the distribution information of each sample represented with fuzzy membership degree, which is incorporated into the redefinition of the scatter matrices. Third, considering the fact that the kernel Fisher discriminant is effective to extract nonlinear discriminative information of the input feature space by using kernel trick, a kernel algorithm based on the new discriminant analysis is presented subsequently, which has the potential to outperform the traditional subspace learning algorithms, especially in the cases of nonlinear small sample sizes. Experimental results conducted on the ORL and Yale face database demonstrate the effectiveness of the proposed method.

Title: Weighted Complete Linear Discriminant Analysis and Its Application to Face Recognition
Abstract: In this paper, we propose a novel weighted complete linear discriminant analysis (WCLDA) method for feature extraction and recognition. The WCLDA first introduces a weighting function to restrain the dominant role of the classes with larger distance and then searches the optimal discriminant vectors under the conjugative orthogonal constrains in the null space of the within-class scatter matrix and its conjugative orthogonal complement space, respectively. As a result, the proposed technique derives the optimal and lossless discriminative information. Experiments on ORL and Yale face database are performed to test and evaluate the proposed algorithm. The results demonstrate the effectiveness of WCLDA.

Title: A Kernel Based Neighborhood Discriminant Submanifold Learning for Pattern Classification
Abstract: We propose a novel method, called Kernel Neighborhood Discriminant Analysis (KNDA), which can be regarded as a supervised kernel extension of Locality Preserving Projection (LPP). KNDA nonlinearly maps the original data into a kernel space in which two graphs are constructed to depict the within-class submanifold and the between-class submanifold. Then a criterion function which minimizes the quotient between the within-class representation and the between-class representation of the submanifolds is designed to separate each submanifold constructed by each class. The real contribution of this paper is that we bring and extend the submanifold based algorithm to a general model and by some derivation a simple result is given by which we can classify a given object to a predefined class effectively. Experiments on the MNIST Handwritten Digits database, the Binary Alphadigits database, the ORL face database, the Extended Yale Face Database B, and a downloaded documents dataset demonstrate the effectiveness and robustness of the proposed method.

Title: Lorentzian discriminant projection and its applications
Abstract: This paper develops a supervised dimensionality reduction method, Lorentzian Discriminant Projection (LDP), for discriminant analysis and classification Our method represents the structures of sample data by a manifold, which is furnished with a Lorentzian metric tensor Different from classic discriminant analysis techniques, LDP uses distances from points to their within-class neighbors and global geometric centroid to model a new manifold to detect the intrinsic local and global geometric structures of data set In this way, both the geometry of a group of classes and global data structures can be learnt from the Lorentzian metric tensor Thus discriminant analysis in the original sample space reduces to metric learning on a Lorentzian manifold The experimental results on benchmark databases demonstrate the effectiveness of our proposed method.

Title: Learning Local Feature Descriptors Using Convex Optimisation
Abstract: The objective of this work is to learn descriptors suitable for the sparse feature detectors used in viewpoint invariant matching. We make a number of novel contributions towards this goal. First, it is shown that learning the pooling regions for the descriptor can be formulated as a convex optimisation problem selecting the regions using sparsity. Second, it is shown that descriptor dimensionality reduction can also be formulated as a convex optimisation problem, using Mahalanobis matrix nuclear norm regularisation. Both formulations are based on discriminative large margin learning constraints. As the third contribution, we evaluate the performance of the compressed descriptors, obtained from the learnt real-valued descriptors by binarisation. Finally, we propose an extension of our learning formulations to a weakly supervised case, which allows us to learn the descriptors from unannotated image collections. It is demonstrated that the new learning methods improve over the state of the art in descriptor learning on the annotated local patches data set of Brown et al. and unannotated photo collections of Philbin et al. .

Title: TWO-Dimensional Linear Discriminant Analysis of Principle Component Vectors for Face Recognition
Abstract: In this paper, we proposed a new two-dimensional linear discriminant analysis (2DLDA) method. Based on two-dimensional principle component analysis (2DPCA), face image matrices do not need to be previously transformed into a vector. In this way, the spatial information can be preserved. Moreover, the 2DLDA also allows avoiding the small sample size (SSS) problem, thus overcoming the traditional LDA. We combine 2DPCA and our proposed 2DLDA on the two-dimensional linear discriminant analysis of principle component vectors framework. Our framework consists of two steps: first we project an input face image into the family of projected vectors via 2DPCA-based technique, second we project from these space into the classification space via 2DLDA-based technique. This does not only allows further reducing of the dimension of the feature matrix but also improving the classification accuracy. Experimental results on ORL and Yale face database showed an improvement of 2DPCA-based technique over the conventional PCA technique

Title: Local Coordinates Alignment and Its Linearization
Abstract: Manifold learning has been demonstrated to be an effective way to discover the intrinsic geometrical structure of a number of samples. In this paper, a new manifold learning algorithm, Local Coordinates Alignment (LCA), is developed based on the alignment technique. LCA first obtains the local coordinates as representations of a local neighborhood by preserving the proximity relations on the patch which is Euclidean; and then the extracted local coordinates are aligned to yield the global embeddings. To solve the out of sample problem, the linearization of LCA (LLCA) is also proposed. Empirical studies on both synthetic data and face images show the effectiveness of LCA and LLCA in comparing with existing manifold learning algorithms and linear subspace methods.

Title: Spectral Regression: A Unified Approach for Sparse Subspace Learning
Abstract: Recently the problem of dimensionality reduction (or, subspace learning) has received a lot of interests in many fields of information processing, including data mining, information retrieval, and pattern recognition. Some popular methods include principal component analysis (PCA), linear discriminant analysis (LDA) and locality preserving projection (LPP). However, a disadvantage of all these approaches is that the learned projective functions are linear combinations of all the original features, thus it is often difficult to interpret the results. In this paper, we propose a novel dimensionality reduction framework, called  Unified   Sparse   Subspace   Learning  (USSL), for learning sparse projections. USSL casts the problem of learning the projective functions into a regression framework, which facilitates the use of different kinds of regularizes. By using a L 1 -norm regularizer (lasso), the sparse projections can be efficiently computed. Experimental results on real world classification and clustering problems demonstrate the effectiveness of our method.

Title: Two-dimensional multiple maximum scatter difference method for face recognition
Abstract: In this paper we propose a two-dimensional multiple maximum scatter difference (2DMMSD) method for face representation and recognition. The new algorithm is based on multiple maximum scatter difference (MMSD) discriminant criterion and image matrices projection technique. The 2DMMSD method does not need to transform image matrix into a vector prior to feature extraction so that it is computationally more efficient and accurate than MMSD in extracting the facial features. Experimental results on ORL and Yale face databases demonstrate the effectiveness and robustness of the proposed method.

Title: Semi-Supervised Bilinear Subspace Learning
Abstract: Recent research has demonstrated the success of tensor based subspace learning in both unsupervised and supervised configurations (e.g., 2-D PCA, 2-D LDA, and DATER). In this correspondence, we present a new semi-supervised subspace learning algorithm by integrating the tensor representation and the complementary information conveyed by unlabeled data. Conventional semi-supervised algorithms mostly impose a regularization term based on the data representation in the original feature space. Instead, we utilize graph Laplacian regularization based on the low-dimensional feature space. An iterative algorithm, referred to as adaptive regularization based semi-supervised discriminant analysis with tensor representation (ARSDA/T), is also developed to compute the solution. In addition to handling tensor data, a vector-based variant (ARSDA/V) is also presented, in which the tensor data are converted into vectors before subspace learning. Comprehensive experiments on the CMU PIE and YALE-B databases demonstrate that ARSDA/T brings significant improvement in face recognition accuracy over both conventional supervised and semi-supervised subspace learning algorithms.

Title: Two-Dimensional Linear Discriminant Analysis of Principle Component Vectors for Face Recognition
Abstract: In this paper, we proposed a new Two-Dimensional Linear Discriminant Analysis (2DLDA) method, based on Two-Dimensional Principle Component Analysis (2DPCA) concept. In particular, 2D face image matrices do not need to be previously transformed into a vector. In this way, the spatial information can be preserved. Moreover, the 2DLDA also allows avoiding the Small Sample Size (SSS) problem, thus overcoming the traditional LDA. We combine 2DPCA and our proposed 2DLDA on the Two-Dimensional Linear Discriminant Analysis of principle component vectors framework. Our framework consists of two steps: first we project an input face image into the family of projected vectors via 2DPCA-based technique, second we project from these space into the classification space via 2DLDA-based technique. This does not only allows further reducing of the dimension of feature matrix but also improving the classification accuracy. Experimental results on ORL and Yale face database showed an improvement of 2DPCA-based technique over the conventional PCA technique.

Title: Constrained discriminant neighborhood embedding for high dimensional data feature extraction
Abstract: When handling pattern classification problem such as face recognition and digital handwriting identification, image data is always represented to high dimensional vectors, from which discriminant features are extracted using dimensionality reduction methods. So in this paper, we present a supervised manifold learning based dimensionality reduction method named constrained discirminant neighborhood embedding (CDNE). In the proposed CDNE, on one hand, the class information of samples is taken into account to construct both an inter-class graph and an intra-class graph, where neighborhood points in the intra-class graph are selected from those with the same class and any point in the inter-class graph should sample those labeled different classes as its neighborhood points. On the other hand, locally least linear reconstruction technique is also introduced to model an objective function under the local uncorrelation constraint to explore a discriminant subspace. Compared to some related and state-of-the-art dimensionality reduction methods such as discriminant neighborhood embedding (DNE), supervised locality discriminant manifold learning (SLDML), discriminant sparse neighborhood preserving embedding (DSNPE), local graph embedding based on maximum margin criterion (LGE/MMC), uncorrelated discriminant locality preserving projection (UDLPP) and locally uncorrelated discriminant projection (LUDP), the proposed CDNE has been validated to be efficient and feasible by experimental results on some benchmark face data sets including CMU PIE, ORL and FERET.

Title: Kernel coupled distance metric learning for gait recognition and face recognition
Abstract: Abstract   The performances of biometrics may be adversely impact by different walking states, walking directions, resolutions of gait sequence images, pose variation and low resolution of face images. To address these problems, we presented a kernel coupled distance metric learning (KCDML) method after considering matching among different data collections. By using a kernel trick and a specialized locality preserving criterion, we formulated the problem of kernel coupled distance metric learning as an optimization problem whose aims are to search for the pair-wise samples staying as close as possible and to preserve the local structure intrinsic data geometry. Instead of an iterative solution, one single generalized eigen-decomposition can be leveraged to compute the two transformation matrices for two classifications of data sets. The effectiveness of the proposed method is empirically demonstrated on gait and face recognition tasks' results which outperform four linear subspace solutions' (i.e. CDML, PCA, LPP, LDA) and four nonlinear subspace solutions' (i.e. Huang's method, PCA-RBF, KPCA, KLPP).

Title: An efficient nonnegative matrix factorization approach in flexible kernel space
Abstract: In this paper, we propose a general formulation for kernel nonnegative matrix factorization with flexible kernels. Specifically, we propose the Gaussian nonnegative matrix factorization (GNMF) algorithm by using the Gaussian kernel in the framework. Different from a recently developed polynomial NMF (PNMF), GNMF finds basis vectors in the kernel-induced feature space and the computational cost is independent of input dimensions. Furthermore, we prove the convergence and nonnegativity of decomposition of our method. Extensive experiments compared with PNMF and other NMF algorithms on several face databases, validate the effectiveness of the proposed method.

Title: Collaborative Sparse Preserving Projections for Feature Extraction
Abstract: Sparsity Preserving Projections (SPP) is a well known approach for feature extraction and dimensionality reduction. Its success is mainly attributed to its high quality graph which is constructed by sparse representation. As an instance of graph embedding, SPP can be formulated as regression model. Thus we apply the idea of collaborative graph embedding, which reformulates SPP as a collaborative representation model via imposing a L2-norm constraint to projections from the perspective of linear regression, to further enhance SPP. We call this novel SPP method Collaborative Sparsity Preserving Projections (CSPP). Experiment results on four popular face datasets, namely Yale, ORL, FERET and AR, show the effectiveness in feature extraction and the improvement of CSPP over SPP.

Title: Orthogonal nonnegative matrix t-factorizations for clustering
Abstract: Currently, most research on nonnegative matrix factorization (NMF)focus on 2-factor $X=FG^T$ factorization. We provide a systematicanalysis of 3-factor $X=FSG^T$ NMF. While it unconstrained 3-factor NMF is equivalent to it unconstrained 2-factor NMF, itconstrained 3-factor NMF brings new features to it constrained 2-factor NMF. We study the orthogonality constraint because it leadsto rigorous clustering interpretation. We provide new rules for updating $F,S, G$ and prove the convergenceof these algorithms. Experiments on 5 datasets and a real world casestudy are performed to show the capability of bi-orthogonal 3-factorNMF on simultaneously clustering rows and columns of the input datamatrix. We provide a new approach of evaluating the quality ofclustering on words using class aggregate distribution andmulti-peak distribution. We also provide an overview of various NMF extensions andexamine their relationships.

Title: Visual search reranking with RElevant Local Discriminant Analysis
Abstract: Visual search reranking is a promising technique to refine the text-based image search results with visual information. Dimensionality reduction is one of the key preprocessing steps in it to overcome the curse of dimensionality brought by the high-dimensional visual features. However, there are few dimensionality reduction algorithms employing the relevance degree information for visual search reranking. This paper proposes a novel dimensionality reduction algorithm called RElevant Local Discriminant Analysis (RELDA) for visual search reranking. As a semi-supervised combination of improved Linear Discriminant Analysis (LDA) and Locality Preserving Projections (LPP), the proposed RELDA algorithm preserves the local manifold structure of the whole data as well as controls the relevance between labeled examples. Moreover, RELDA algorithm has an analytic form of the globally optimal solution and can be computed based on eigen-decomposition. Extensive experiments on two popular real-world visual search reranking datasets demonstrate the superiority of the proposed RELDA algorithm.

Title: Face recognition using second-order discriminant tensor subspace analysis
Abstract: Discriminant information (DI) plays a critical role in face recognition. In this paper, we proposed a second-order discriminant tensor subspace analysis (DTSA) algorithm to extract discriminant features from the intrinsic manifold structure of the tensor data. DTSA combines the advantages of previous methods with DI, the tensor methods preserving the spatial structure information of the original image matrices, and the manifold methods preserving the local structure of the samples distribution. DTSA defines two similarity matrices, namely within-class similarity matrix and between-class similarity matrix. The within-class similarity matrix is determined by the distances of point pairs in the same class, while the between-class similarity matrix is determined by the distances between the means of each pair of classes. Using these two matrices, the proposed method preserves the local structure of the samples to fit the manifold structure of facial images in high dimensional space better than other methods. Moreover, compared to the 2D methods, the tensor based method employs two-sided transformations rather than single-sided one, and yields higher compression ratio. As a tensor method, DTSA uses an iterative procedure to calculate the optimal solution of two transformation matrices. In this paper, we analyzed DTSA's connections to 2D-DLPP and TSA, theoretically. The experiments on the ORL, Yale and YaleB facial databases show the effectiveness of the proposed method.

Title: Schatten p-norm based principal component analysis
Abstract: Structured sparse PCA (SSPCA) is a new emerging method regularized by structured sparsity-inducing norms. However, these regularization terms are not necessarily optimal because of the noisy and irrelevant features embedded in predefined patterns. This paper presents a method called Schatten p-norm based principal component analysis (SpPCA) to learn interpretable and structured elements (or factors). In SpPCA, a low-rank assumption is used to characterize structured elements in a two-dimensional matrix form. Compared to SSPCA, the low-rank assumption of SpPCA is more intuitive and effective for describing object parts of an image. Moreover, SpPCA can deal with some scenarios, where the dictionary element matrixes have complex structures. We also propose an efficient and simple optimization procedure to solve the problem. Extensive experiments on denoising of sparse structured signals and face recognition on different databases (e.g. AR, Extend Yale B and Multi-PIE) demonstrate the superior performance over some recently proposed methods.

Title: Incremental manifold learning via tangent space alignment
Abstract: Several algorithms have been proposed to analysis the structure of high-dimensional data based on the notion of manifold learning. They have been used to extract the intrinsic characteristic of different type of high-dimensional data by performing nonlinear dimensionality reduction. Most of them operate in a “batch” mode and cannot be efficiently applied when data are collected sequentially. In this paper, we proposed an incremental version (ILTSA) of LTSA (Local Tangent Space Alignment), which is one of the key manifold learning algorithms. Besides, a landmark version of LTSA (LLTSA) is proposed, where landmarks are selected based on LASSO regression, which is well known to favor sparse approximations because it uses regularization with l1 norm. Furthermore, an incremental version (ILLTSA) of LLTSA is also proposed. Experimental results on synthetic data and real word data sets demonstrate the effectivity of our algorithms.

Title: Joint Global and Local Structure Discriminant Analysis
Abstract: Linear discriminant analysis (LDA) only considers the global Euclidean geometrical structure of data for dimensionality reduction. However, previous works have demonstrated that the local geometrical structure is effective for dimensionality reduction. In this paper, a novel approach is proposed, namely Joint Global and Local-structure Discriminant Analysis (JGLDA), for linear dimensionality reduction. To be specific, we construct two adjacency graphs to represent the local intrinsic structure, which characterizes both the similarity and diversity of data, and integrate the local intrinsic structure into Fisher linear discriminant analysis to build a stable discriminant objective function for dimensionality reduction. Experiments on several standard image databases demonstrate the effectiveness of our algorithm.

