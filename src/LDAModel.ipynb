{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CCF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYHgrBp-5QJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "1d565134-bcbd-4133-e37d-3d0ba77df398"
      },
      "source": [
        "# install nltk and download language packages\n",
        "!python3 -m nltk.downloader wordnet punkt averaged_perceptron_tagger"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkzgqai-5YdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from util import load_json, dump_json\n",
        "\n",
        "seed = 17"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb8kir6m9ljs",
        "colab_type": "code",
        "outputId": "a432990b-a601-4eba-fc57-4507e6f4f2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# text processing and cleanup\n",
        "!python3 preprocess_data.py -a output.txt -o data.json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "868it [00:10, 86.15it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um08talo5fCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "candidates = load_json('candidates.json')\n",
        "candidates = {list(c.keys())[0]:c[list(c.keys())[0]] for c in candidates}\n",
        "assert len(candidates.keys()) == len(load_json('candidates.json'))\n",
        "# put data into a pandas\n",
        "articles = load_json('data.json')\n",
        "article_properties = list(sorted(articles[0].keys()))\n",
        "articles_dict = {article_property: [article[article_property] for article in articles] for article_property in article_properties}\n",
        "articles_df = pd.DataFrame.from_dict(articles_dict)\n",
        "articles_train_df, articles_test_df = train_test_split(articles_df, test_size=0.25, random_state=17)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skdq7VOM7qwG",
        "colab_type": "code",
        "outputId": "e169a448-ba66-449a-f774-23eb8fa63064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "articles_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>fos</th>\n",
              "      <th>id</th>\n",
              "      <th>references</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>company diverse industry decide pricing policy...</td>\n",
              "      <td>[economics, profitability index, microeconomic...</td>\n",
              "      <td>100913566</td>\n",
              "      <td>[1502705241, 2002741233, 2010497485, 205624824...</td>\n",
              "      <td>Contingent Preannounced Pricing Policies with ...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>emerge ad hoc cloud form cloud compute paradig...</td>\n",
              "      <td>[computer science, distribute computing, algeb...</td>\n",
              "      <td>1009626802</td>\n",
              "      <td>[116648389, 1506342804, 1535487433, 1601098769...</td>\n",
              "      <td>A Distributed Secure Outsourcing Scheme for So...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>article study univariate bivariate truncate vo...</td>\n",
              "      <td>[anderson darling test, artificial intelligenc...</td>\n",
              "      <td>101132528</td>\n",
              "      <td>[2135642594]</td>\n",
              "      <td>Univariate and bivariate truncated von Mises d...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sequence cluster important topic expert data m...</td>\n",
              "      <td>[fuzzy clustering, correlation cluster, cluste...</td>\n",
              "      <td>1011638332</td>\n",
              "      <td>[1599310603, 2007370139, 2027499598, 205954804...</td>\n",
              "      <td>Sequence clustering algorithm based on weighte...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>propose principled approach learn parameter ba...</td>\n",
              "      <td>[wake sleep algorithm, semi supervise learning...</td>\n",
              "      <td>1013235724</td>\n",
              "      <td>[162422970, 203049729, 293230287, 1503398984, ...</td>\n",
              "      <td>Learning Bayesian network parameters under equ...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract  ...  year\n",
              "0  company diverse industry decide pricing policy...  ...  2016\n",
              "1  emerge ad hoc cloud form cloud compute paradig...  ...  2017\n",
              "2  article study univariate bivariate truncate vo...  ...  2017\n",
              "3  sequence cluster important topic expert data m...  ...  2017\n",
              "4  propose principled approach learn parameter ba...  ...  2017\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umbOwwnOEr9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "candidates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b35MxCI6C6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count occurrences of words in training data\n",
        "count_vectorizer = CountVectorizer()\n",
        "corpus = [article['abstract'] for _, article in articles_train_df.iterrows()]\n",
        "count_vectorizer.fit(corpus)\n",
        "\n",
        "# create a dataframe of counts slowly to not fill up the memory\n",
        "step_size = 1000\n",
        "count_cols = ['vocab_{}'.format(word) for word in count_vectorizer.get_feature_names()]\n",
        "counts_df = pd.DataFrame(columns=count_cols)\n",
        "for i in range(0, len(articles_train_df), step_size):\n",
        "  corpus = [article['abstract'] for _, article in articles_train_df.iloc[[k for k in range(i, min(i+step_size+1, len(articles_train_df)))]].iterrows()]\n",
        "  counts = count_vectorizer.transform(corpus)\n",
        "  step_counts_df = pd.DataFrame(counts.todense(), columns=count_cols)\n",
        "  counts_df = pd.concat([counts_df, step_counts_df], ignore_index=True).reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iWoDITMWRvM",
        "colab_type": "code",
        "outputId": "0324d645-2fce-4e6e-a969-93c08fcc419c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "counts_df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(651, 6541)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOypeDUnQ3cB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the counts dataframe to training dataframe\n",
        "train_counts_df = pd.concat([articles_train_df, counts_df], axis=1).reset_index()\n",
        "train_counts_no_nans_df = train_counts_df[count_cols].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB7VrQB44UHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do the same thing for test, except use the same vectorizer as train\n",
        "counts_df = pd.DataFrame(columns=count_cols)\n",
        "for i in range(0, len(articles_test_df), step_size):\n",
        "  corpus = [article['abstract'] for _, article in articles_test_df.iloc[[k for k in range(i, min(i+step_size+1, len(articles_test_df)))]].iterrows()]\n",
        "  counts = count_vectorizer.transform(corpus)\n",
        "  step_counts_df = pd.DataFrame(counts.todense(), columns=count_cols)\n",
        "  counts_df = pd.concat([counts_df, step_counts_df], ignore_index=True).reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn7Op0yr4bne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the counts dataframe to test dataframe\n",
        "test_counts_df = pd.concat([articles_test_df, counts_df], axis=1).reset_index()\n",
        "test_counts_no_nans_df = test_counts_df[count_cols].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrm6IGOVTBqW",
        "colab_type": "code",
        "outputId": "6a0563f6-af13-4c2e-c6a8-f04eb8425978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check for nulls\n",
        "train_counts_no_nans_df[count_cols].isnull().sum().sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWEN09cRFZh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_candidates = {paper_id: candidates[paper_id] for paper_id in candidates if paper_id in test_counts_df['id'].tolist()}\n",
        "train_candidates = {paper_id: candidates[paper_id] for paper_id in candidates if paper_id in train_counts_df['id'].tolist()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AowKxMg1FIy-",
        "colab_type": "code",
        "outputId": "b35accb0-0e03-47fc-bad2-7ba29c042ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vocabulary size\n",
        "len(count_vectorizer.get_feature_names())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6540"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stW8gQ_9J172",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_counts_no_nans_df = train_counts_no_nans_df.reset_index()\n",
        "test_counts_no_nans_df = test_counts_no_nans_df.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hnoiw3GcbnSC",
        "colab": {}
      },
      "source": [
        "# function that will keep score for a given k for top k candidates\n",
        "def get_test_score(model, train_df, test_df, scores, candidates, top_k=1):\n",
        "  # check if top_k vectors with highest cosine similarity have one of the cited papers\n",
        "  score = 0\n",
        "  num_examples = len(test_df)\n",
        "  score_indices = np.argsort(scores, axis=1)\n",
        "  for i, (_, row) in enumerate(test_df.iterrows()):\n",
        "    if row['id'] in candidates:\n",
        "      candidate_indices = train_df.index[train_df['id'].isin(candidates[row['id']])].tolist()\n",
        "      candidates_df = train_df.iloc[candidate_indices]\n",
        "      candidate_scores = scores[i, candidate_indices]\n",
        "      top_k_candidate_scores_indices = np.argsort(candidate_scores)[::-1][:top_k]\n",
        "      top_k_ids = candidates_df.iloc[top_k_candidate_scores_indices]['id'].tolist()\n",
        "      if any(citation_id in top_k_ids for citation_id in row['references']):\n",
        "        score += 1\n",
        "      elif not any(candidates_df['id'].isin(row['references'])):\n",
        "        # none of the candidates were valid\n",
        "        # invalid example\n",
        "        num_examples -= 1\n",
        "    else:\n",
        "      # current paper is not in the candidates\n",
        "      # invalid example\n",
        "      num_examples -= 1\n",
        "\n",
        "  if num_examples == 0:\n",
        "    return 1\n",
        "\n",
        "  return score / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3K-Phn16tQO",
        "colab_type": "code",
        "outputId": "545c55d8-1f29-4a8a-aa9c-2f3bcf895455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# CV grid search for best n_components\n",
        "n_splits = 2\n",
        "kf = KFold(n_splits=n_splits, random_state=seed)\n",
        "scores = []\n",
        "base_n_components = 12\n",
        "for n in range(base_n_components, 15):\n",
        "  fold_scores = []\n",
        "  for i, (train_index, test_index) in enumerate(kf.split(train_counts_no_nans_df)):\n",
        "    train_df = train_counts_no_nans_df.iloc[train_index]\n",
        "    validate_df = train_counts_no_nans_df.iloc[test_index]\n",
        "    lda = LatentDirichletAllocation(n_components=n, random_state=seed)\n",
        "    lda.fit(train_df[count_cols])\n",
        "    \n",
        "    X = lda.transform(train_counts_no_nans_df[count_cols])\n",
        "    X_validate = lda.transform(validate_df[count_cols])\n",
        "    score = get_test_score(lda, train_counts_df, train_counts_df.iloc[test_index], np.matmul(X_validate, X.T), train_candidates, 1)\n",
        "    fold_scores += [score]\n",
        "    print('N:', n, 'Score:', '{}%'.format(score*100))\n",
        "\n",
        "  scores += [sum(fold_scores) / len(fold_scores)]\n",
        "\n",
        "# found the best n\n",
        "best_n_components = base_n_components + scores.index(max(scores))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N: 12 Score: 80.51948051948052%\n",
            "N: 12 Score: 100%\n",
            "N: 13 Score: 81.81818181818183%\n",
            "N: 13 Score: 100%\n",
            "N: 14 Score: 79.22077922077922%\n",
            "N: 14 Score: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyerBcSZ7W3-",
        "colab_type": "code",
        "outputId": "dddbb07c-6130-4a91-b938-63a75da7d328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# evaluate lda\n",
        "best_lda = LatentDirichletAllocation(n_components=best_n_components, random_state=seed)\n",
        "best_lda.fit(train_counts_no_nans_df[count_cols])\n",
        "X_train = best_lda.fit_transform(train_counts_no_nans_df[count_cols])\n",
        "X_test = best_lda.transform(test_counts_no_nans_df[count_cols])\n",
        "scores = np.matmul(X_test, X_train.T)\n",
        "print('Top 1:', '{}%'.format(get_test_score(best_lda, train_counts_df, test_counts_df, np.matmul(X_test, X_train.T), test_candidates, top_k=1)*100))\n",
        "print('Top 2:', '{}%'.format(get_test_score(best_lda, train_counts_df, test_counts_df, np.matmul(X_test, X_train.T), test_candidates, top_k=2)*100))\n",
        "print('Top 3:', '{}%'.format(get_test_score(best_lda, train_counts_df, test_counts_df, np.matmul(X_test, X_train.T), test_candidates, top_k=3)*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 1: 72.22222222222221%\n",
            "Top 2: 72.22222222222221%\n",
            "Top 3: 94.44444444444444%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}